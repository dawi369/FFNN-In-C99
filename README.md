# ğŸ§  Feedforward Neural Network in C99

Welcome to the **Feedforward Neural Network** project! This project is a simple yet powerful implementation of a **neural network** built in **C99** to classify **28x28 pixel images of digits**. ğŸ“Š

PS. Everything I do is on WSL2 runnig Debian 12 (Bookworm)

---

## ğŸš€ Features

- ğŸ–¼ï¸ **Image Input**: 28x28 grayscale images (e.g., from the MNIST dataset).
- âš¡ **Activation Function**: ReLU (Rectified Linear Unit) for non-linear activation.
- ğŸ”„ **Learning Method**: Backpropagation for optimizing the network.
- ğŸ§® **Loss Function**: Cross-entropy to calculate the model's performance.
- ğŸ”§ **Adjustable Hyperparameters**: Easily customize layer sizes, learning rates, etc.

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ main.c       
â”‚   â””â”€â”€ softmax.c
â”œâ”€â”€ include
â”‚   â””â”€â”€ softmax.h
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ data_organizer.py
â”‚   â”œâ”€â”€ test_images.txt
â”‚   â”œâ”€â”€ test_lables.txt
â”‚   â”œâ”€â”€ training_images.txt
â”‚   â””â”€â”€ training_lables.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ makefile
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md        # You're looking at it! ğŸ“„
```

## ğŸ› ï¸ Installation
1. Clone the Repository:
```bash
git clone https://github.com/username/feedforward-neural-network.git
cd feedforward-neural-network
```
2. Run the makefile:
```bash
make
```
OR

Compile the Code:
Make sure you have a C99-compatible compiler (like gcc):
```bash
gcc -std=c99 -o prg src/main.c src/softmax.c -Iinclude -lm
```
3. Run the Neural Network:
```bash
./prg
```
---

## ğŸ§‘â€ğŸ« How It Works

1. **Input Layer**:  
   The network takes **28x28 pixel images** (flattened to a **784-dimensional vector**) as input. Each pixel is normalized to the range [0, 1].

2. **Hidden Layers**:  
   Uses **fully connected hidden layers** with **ReLU activation**. The number of hidden layers and neurons can be easily adjusted in the configuration.

3. **Output Layer**:  
   The output layer contains **10 neurons** corresponding to the 10 digit classes (0-9). The **softmax function** is applied to convert raw scores into probabilities.

4. **Training**:  
   The network uses **backpropagation** and the **cross-entropy loss function** to update weights and biases during training.

---

## ğŸ“Š Performance

ğŸ§‘â€ğŸ’» After training for several epochs, the neural network can achieve **high accuracy** on the test set of digit images. You can tweak the learning rate, number of layers, or number of neurons to improve performance.

---

## ğŸ› ï¸ Hyperparameters

- **Learning Rate**: Adjustable in the `main.c`.
- **Number of Hidden Layers**: Configurable in the `main.c`.
- **Number of Neurons**: You can set the number of neurons for each hidden layer.

---

## ğŸ“ˆ Results

After training, you should expect to see results like:

- **Training Accuracy**: ~98%
- **Test Accuracy**: ~97%

---

## ğŸ§‘â€ğŸ’» Contributing

We welcome contributions from the community! Feel free to submit **pull requests** or open **issues**.

---

## ğŸ“ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

Happy coding! ğŸ˜
















